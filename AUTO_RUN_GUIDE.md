# ğŸ¤– ìë™ ì‹¤í–‰ ê°€ì´ë“œ

AI Content CollectorëŠ” **ë‘ ê°€ì§€ ëª¨ë“œ**ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **MCP ì„œë²„ ëª¨ë“œ** - Claude Desktopê³¼ ì—°ë™í•˜ì—¬ ëŒ€í™”ë¡œ ì œì–´
2. **ìë™ í¬ë¡¤ëŸ¬ ëª¨ë“œ** â­ - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìë™ìœ¼ë¡œ ì½˜í…ì¸  ìˆ˜ì§‘

---

## ğŸ¯ ìë™ í¬ë¡¤ëŸ¬ ëª¨ë“œ (ì¶”ì²œ)

ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì£¼ê¸°ì ìœ¼ë¡œ ìë™ ì‹¤í–‰ë˜ëŠ” í¬ë¡¤ëŸ¬ì…ë‹ˆë‹¤.

### ì‘ë™ ë°©ì‹

1. **DBì—ì„œ ì¹´í…Œê³ ë¦¬ ìë™ ë¡œë“œ**
   - Category í…Œì´ë¸”ì˜ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì½ê¸°
   - ìƒˆ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ì‹œ ìë™ ë°˜ì˜

2. **ê° ì¹´í…Œê³ ë¦¬ë³„ ìë™ ìˆ˜ì§‘**
   - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì„¤ì •ëœ ê°œìˆ˜ë§Œí¼ ì½˜í…ì¸  ìˆ˜ì§‘
   - ê²€ìƒ‰ â†’ ìŠ¤í¬ë˜í•‘ â†’ AI ë¶„ì„ â†’ DB ì €ì¥

3. **Storage ìë™ ì—…ë¡œë“œ**
   - ì¸ë„¤ì¼ â†’ `contents_thumbnail` ë²„í‚·
   - AI Tool ë¡œê³  â†’ `ai-tool-logos` ë²„í‚·

4. **ì¤‘ë³µ ìë™ ìŠ¤í‚µ**
   - sourceUrl ê¸°ì¤€ ì¤‘ë³µ ì²´í¬
   - ì´ë¯¸ ìˆëŠ” ì½˜í…ì¸ ëŠ” ê±´ë„ˆëœ€

5. **ì£¼ê¸°ì  ë°˜ë³µ**
   - ì„¤ì •ëœ ê°„ê²©(ê¸°ë³¸ 24ì‹œê°„)ë§ˆë‹¤ ìë™ ì‹¤í–‰

---

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### ë°©ë²• 1: ì§ì ‘ ì‹¤í–‰ (ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸)

```bash
# í•œ ë²ˆë§Œ ì‹¤í–‰
npm run crawler:once

# ì§€ì†ì ìœ¼ë¡œ ì‹¤í–‰ (24ì‹œê°„ë§ˆë‹¤ ë°˜ë³µ)
npm run crawler
```

### ë°©ë²• 2: systemd ì„œë¹„ìŠ¤ (Linux ì„œë²„, ê¶Œì¥)

ì„œë²„ ë¶€íŒ… ì‹œ ìë™ ì‹œì‘ë˜ê³  í¬ë˜ì‹œ ì‹œ ìë™ ì¬ì‹œì‘ë©ë‹ˆë‹¤.

#### 1. ì„œë¹„ìŠ¤ íŒŒì¼ ì„¤ì •

```bash
# ì„œë¹„ìŠ¤ íŒŒì¼ í¸ì§‘
nano ai-content-crawler.service

# ë‹¤ìŒ í•­ëª© ìˆ˜ì •:
# - User=your-username (ì‹¤ì œ ì‚¬ìš©ìëª…)
# - WorkingDirectory=/ì ˆëŒ€/ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œ)
# - ExecStart=/ì ˆëŒ€/ê²½ë¡œ (ì‹¤ì œ ê²½ë¡œ)
# - Environment ë³€ìˆ˜ë“¤ (ì‹¤ì œ API í‚¤)
```

#### 2. systemdì— ë“±ë¡

```bash
# ì„œë¹„ìŠ¤ íŒŒì¼ ë³µì‚¬
sudo cp ai-content-crawler.service /etc/systemd/system/

# systemd ë¦¬ë¡œë“œ
sudo systemctl daemon-reload

# ì„œë¹„ìŠ¤ í™œì„±í™” (ë¶€íŒ… ì‹œ ìë™ ì‹œì‘)
sudo systemctl enable ai-content-crawler

# ì„œë¹„ìŠ¤ ì‹œì‘
sudo systemctl start ai-content-crawler

# ìƒíƒœ í™•ì¸
sudo systemctl status ai-content-crawler

# ë¡œê·¸ í™•ì¸
sudo journalctl -u ai-content-crawler -f
```

#### 3. ì„œë¹„ìŠ¤ ê´€ë¦¬

```bash
# ì¤‘ì§€
sudo systemctl stop ai-content-crawler

# ì¬ì‹œì‘
sudo systemctl restart ai-content-crawler

# ë¹„í™œì„±í™” (ë¶€íŒ… ì‹œ ìë™ ì‹œì‘ í•´ì œ)
sudo systemctl disable ai-content-crawler
```

### ë°©ë²• 3: Docker Compose (ê¶Œì¥, ê²©ë¦¬ëœ í™˜ê²½)

Docker ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰í•˜ì—¬ ì‹œìŠ¤í…œê³¼ ê²©ë¦¬í•©ë‹ˆë‹¤.

#### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# .env íŒŒì¼ ìƒì„±
cp .env.example .env

# ì‹¤ì œ API í‚¤ ì…ë ¥
nano .env
```

#### 2. Docker ì‹¤í–‰

```bash
# ë¹Œë“œ ë° ì‹œì‘
docker-compose up -d

# ë¡œê·¸ í™•ì¸
docker-compose logs -f

# ì¤‘ì§€
docker-compose down

# ì¬ì‹œì‘
docker-compose restart
```

### ë°©ë²• 4: PM2 (Node.js í”„ë¡œì„¸ìŠ¤ ë§¤ë‹ˆì €)

```bash
# PM2 ì„¤ì¹˜
npm install -g pm2

# í¬ë¡¤ëŸ¬ ì‹œì‘
pm2 start dist/crawler.js --name ai-content-crawler

# ë¶€íŒ… ì‹œ ìë™ ì‹œì‘ ì„¤ì •
pm2 startup
pm2 save

# ìƒíƒœ í™•ì¸
pm2 status

# ë¡œê·¸ í™•ì¸
pm2 logs ai-content-crawler

# ì¤‘ì§€
pm2 stop ai-content-crawler

# ì¬ì‹œì‘
pm2 restart ai-content-crawler
```

---

## âš™ï¸ ì„¤ì • ì˜µì…˜

### í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ | ê¸°ë³¸ê°’ | ì„¤ëª… |
|------|--------|------|
| `ITEMS_PER_CATEGORY` | 5 | ê° ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê°œìˆ˜ |
| `CRAWL_INTERVAL_HOURS` | 24 | í¬ë¡¤ë§ ë°˜ë³µ ì£¼ê¸° (ì‹œê°„) |
| `RUN_ONCE` | false | trueë©´ í•œ ë²ˆë§Œ ì‹¤í–‰ í›„ ì¢…ë£Œ |

### ì˜ˆì‹œ

```bash
# ê° ì¹´í…Œê³ ë¦¬ë³„ 10ê°œì”©, 12ì‹œê°„ë§ˆë‹¤
ITEMS_PER_CATEGORY=10 CRAWL_INTERVAL_HOURS=12 npm run crawler

# ê° ì¹´í…Œê³ ë¦¬ë³„ 3ê°œì”©, í•œ ë²ˆë§Œ ì‹¤í–‰
ITEMS_PER_CATEGORY=3 RUN_ONCE=true npm run crawler
```

---

## ğŸ“Š ì‹¤í–‰ ê²°ê³¼ í™•ì¸

### ì½˜ì†” ì¶œë ¥

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   AI Content Collector - Auto Crawler  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸš€ Starting AI Content Crawler
â° Time: 2026-01-19T12:00:00.000Z
ğŸ“Š Items per category: 5

âœ… Supabase initialized

ğŸ“‹ Found 4 categories in database:
  - ë°”ì´ë¸Œ ì½”ë”©
  - ì´ë¯¸ì§€ ìƒì„±
  - í…ìŠ¤íŠ¸ ìƒì„±
  - ì˜ìƒ ìƒì„±

ğŸ“‚ Crawling category: ë°”ì´ë¸Œ ì½”ë”©
Target: 5 items
ğŸ” Search query: "AI coding tutorial programming"
Found 10 search results

  ğŸ“„ Processing: How to use AI for coding automation...
  ğŸ“¥ Scraping...
  ğŸ¤– Analyzing with AI...
  ğŸ’¾ Saving to database...
  âœ… Saved! ID: CONTENTS-00123
  ğŸ“Š Difficulty: INTERMEDIATE | Tools: ChatGPT, Cursor

âœ… Category "ë°”ì´ë¸Œ ì½”ë”©" completed: 5/5 items saved

...

ğŸ“Š â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
           CRAWL STATISTICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ” Total Searched:        40
ğŸ“¥ Total Scraped:         20
ğŸ¤– Total Analyzed:        20
ğŸ’¾ Total Saved:           20
â­ï¸  Duplicates Skipped:   8
âŒ Errors:                0
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ… Crawler completed successfully
â° Next run scheduled in 24 hours
```

### Supabaseì—ì„œ í™•ì¸

1. [supabase.com](https://supabase.com) ë¡œê·¸ì¸
2. í”„ë¡œì íŠ¸ ì„ íƒ
3. **Table Editor** â†’ **Content** í…Œì´ë¸”
4. ìƒˆë¡œ ì¶”ê°€ëœ ë°ì´í„° í™•ì¸

### Storage í™•ì¸

1. **Storage** â†’ **contents_thumbnail**
   - ì½˜í…ì¸  ì¸ë„¤ì¼ ì´ë¯¸ì§€ í™•ì¸

2. **Storage** â†’ **ai-tool-logos**
   - AI ë„êµ¬ ë¡œê³  ì´ë¯¸ì§€ í™•ì¸

---

## ğŸ¨ ì¹´í…Œê³ ë¦¬ ì¶”ê°€ ë°©ë²•

### 1. Supabaseì—ì„œ ì§ì ‘ ì¶”ê°€

```sql
-- Category í…Œì´ë¸”ì— ìƒˆ ì¹´í…Œê³ ë¦¬ ì¶”ê°€
INSERT INTO "Category" (id, name, slug, "iconUrl")
VALUES 
  ('cat-xyz123', 'í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§', 'prompt-engineering', NULL);
```

### 2. ìë™ ë°˜ì˜

ë‹¤ìŒ í¬ë¡¤ë§ ì‹¤í–‰ ì‹œ ìë™ìœ¼ë¡œ ìƒˆ ì¹´í…Œê³ ë¦¬ ê°ì§€í•˜ì—¬ ìˆ˜ì§‘ ì‹œì‘!

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### í¬ë¡¤ëŸ¬ê°€ ì‹œì‘ë˜ì§€ ì•ŠìŒ

```bash
# í™˜ê²½ ë³€ìˆ˜ í™•ì¸
echo $SUPABASE_URL
echo $GEMINI_API_KEY

# ë¹Œë“œ í™•ì¸
npm run build

# ìˆ˜ë™ ì‹¤í–‰ìœ¼ë¡œ ì˜¤ë¥˜ í™•ì¸
npm run crawler:once
```

### ì¤‘ë³µ ì½˜í…ì¸ ë§Œ ê³„ì† ë‚˜ì˜´

- ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ë„ˆë¬´ ì¼ë°˜ì ì¼ ìˆ˜ ìˆìŒ
- `src/crawler.ts`ì˜ `generateSearchQuery()` í•¨ìˆ˜ ìˆ˜ì •
- ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì–´ ì‚¬ìš©

### API í• ë‹¹ëŸ‰ ì´ˆê³¼

```
Error: YouTube API quota exceeded
```

**í•´ê²°:**
- CRAWL_INTERVAL_HOURS ì¦ê°€ (ì˜ˆ: 48ì‹œê°„)
- ITEMS_PER_CATEGORY ê°ì†Œ (ì˜ˆ: 3ê°œ)
- API í‚¤ ì¶”ê°€ ë°œê¸‰

### Storage ì—…ë¡œë“œ ì‹¤íŒ¨

- Supabase Storage ë²„í‚· ì¡´ì¬ í™•ì¸
  - `contents_thumbnail` (PUBLIC)
  - `ai-tool-logos` (PUBLIC)
- RLS ì •ì±… í™•ì¸ (anon ì—­í• ì— INSERT ê¶Œí•œ)

---

## ğŸ’¡ Pro Tips

### Tip 1: ë‹¨ê³„ì  ì‹¤í–‰

```bash
# 1ë‹¨ê³„: í…ŒìŠ¤íŠ¸ (1ê°œì”©ë§Œ ìˆ˜ì§‘)
ITEMS_PER_CATEGORY=1 RUN_ONCE=true npm run crawler

# 2ë‹¨ê³„: í™•ì¸ í›„ ì¦ê°€
ITEMS_PER_CATEGORY=5 RUN_ONCE=true npm run crawler

# 3ë‹¨ê³„: ìë™í™”
npm run crawler
```

### Tip 2: ë¡œê·¸ ëª¨ë‹ˆí„°ë§

```bash
# systemd
sudo journalctl -u ai-content-crawler -f

# Docker
docker-compose logs -f

# PM2
pm2 logs ai-content-crawler --lines 100
```

### Tip 3: í¬ë¡ ì¡ìœ¼ë¡œ ë°±ì—…

```bash
# ë§¤ì¼ ìƒˆë²½ 3ì‹œì— í•œ ë²ˆì”© ì‹¤í–‰
0 3 * * * cd /path/to/ai-content-collector-mcp-server && npm run crawler:once
```

### Tip 4: ì•Œë¦¼ ì„¤ì •

í¬ë¡¤ë§ ì™„ë£Œ/ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ ë°›ê¸° (Slack, Discord ë“±)
â†’ `src/crawler.ts`ì˜ `runCrawler()` í•¨ìˆ˜ ëì— ì›¹í›… ì¶”ê°€

---

## ğŸ‰ ì™„ì„±!

ì´ì œ **ì™„ì „ ìë™í™”**ë˜ì—ˆìŠµë‹ˆë‹¤!

- âœ… ì„œë²„ ë¶€íŒ… ì‹œ ìë™ ì‹œì‘
- âœ… í¬ë˜ì‹œ ì‹œ ìë™ ì¬ì‹œì‘
- âœ… ì£¼ê¸°ì ìœ¼ë¡œ ìë™ ìˆ˜ì§‘
- âœ… ìƒˆ ì¹´í…Œê³ ë¦¬ ìë™ ë°˜ì˜
- âœ… ì¤‘ë³µ ìë™ ìŠ¤í‚µ
- âœ… Storage ìë™ ì—…ë¡œë“œ

**ì„¤ì •ë§Œ í•˜ë©´ ì† ëŒˆ í•„ìš” ì—†ì´ ê³„ì† ì½˜í…ì¸ ê°€ ìŒ“ì…ë‹ˆë‹¤! ğŸš€**
